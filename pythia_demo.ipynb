{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pythia_demo.ipynb","version":"0.3.2","provenance":[{"file_id":"1Z9fsh10rFtgWe4uy8nvU4mQmqdokdIRR","timestamp":1565073416708},{"file_id":"1Vlo9ZOxnSG7JpLi6NpCbeGNAQqsRaBI8","timestamp":1556560302564}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kXeEePmeEuuY","colab_type":"text"},"source":["## First download all of the necessary data\n","\n","---\n","\n","Press \"Shift + Enter\" to run each cell sequentially. Alternatively, you can press \"Cmd/Ctrl + F9\" to run all cells and then scroll down to bottom cell."]},{"cell_type":"markdown","metadata":{"id":"G1VeJmWa0HLM","colab_type":"text"},"source":["=== The point of this SW is to: Import image: Ask Question about Image. ==="]},{"cell_type":"code","metadata":{"id":"HRUp0r_-B9N0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d4b9f5a9-1692-4c13-feff-dfbe85a5efeb","executionInfo":{"status":"ok","timestamp":1565074542537,"user_tz":-180,"elapsed":131991,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["# Download Pre-requisites needed for running the e2e model\n","%cd /content/\n","\n","%mkdir model_data\n","!wget -O /content/model_data/answers_vqa.txt https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\n","!wget -O /content/model_data/vocabulary_100k.txt https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt\n","!wget -O /content/model_data/detectron_model.pth  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth \n","!wget -O /content/model_data/pythia.pth https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth\n","!wget -O /content/model_data/pythia.yaml https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.yml\n","!wget -O /content/model_data/detectron_model.yaml https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n","!wget -O /content/model_data/detectron_weights.tar.gz https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz\n","!tar xf /content/model_data/detectron_weights.tar.gz"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content\n","mkdir: cannot create directory ‘model_data’: File exists\n","--2019-08-06 06:53:34--  https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24768 (24K) [text/plain]\n","Saving to: ‘/content/model_data/answers_vqa.txt’\n","\n","/content/model_data 100%[===================>]  24.19K  --.-KB/s    in 0.1s    \n","\n","2019-08-06 06:53:35 (162 KB/s) - ‘/content/model_data/answers_vqa.txt’ saved [24768/24768]\n","\n","--2019-08-06 06:53:37--  https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 626738 (612K) [text/plain]\n","Saving to: ‘/content/model_data/vocabulary_100k.txt’\n","\n","/content/model_data 100%[===================>] 612.05K   803KB/s    in 0.8s    \n","\n","2019-08-06 06:53:38 (803 KB/s) - ‘/content/model_data/vocabulary_100k.txt’ saved [626738/626738]\n","\n","--2019-08-06 06:53:40--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 684079216 (652M) [application/octet-stream]\n","Saving to: ‘/content/model_data/detectron_model.pth’\n","\n","/content/model_data 100%[===================>] 652.39M  12.8MB/s    in 52s     \n","\n","2019-08-06 06:54:33 (12.5 MB/s) - ‘/content/model_data/detectron_model.pth’ saved [684079216/684079216]\n","\n","--2019-08-06 06:54:34--  https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 713440524 (680M) [application/octet-stream]\n","Saving to: ‘/content/model_data/pythia.pth’\n","\n","/content/model_data 100%[===================>] 680.39M  13.0MB/s    in 55s     \n","\n","2019-08-06 06:55:30 (12.5 MB/s) - ‘/content/model_data/pythia.pth’ saved [713440524/713440524]\n","\n","--2019-08-06 06:55:31--  https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.yml\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6577 (6.4K) [text/plain]\n","Saving to: ‘/content/model_data/pythia.yaml’\n","\n","/content/model_data 100%[===================>]   6.42K  --.-KB/s    in 0s      \n","\n","2019-08-06 06:55:33 (43.0 MB/s) - ‘/content/model_data/pythia.yaml’ saved [6577/6577]\n","\n","--2019-08-06 06:55:34--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 918 [text/plain]\n","Saving to: ‘/content/model_data/detectron_model.yaml’\n","\n","/content/model_data 100%[===================>]     918  --.-KB/s    in 0s      \n","\n","2019-08-06 06:55:35 (10.7 MB/s) - ‘/content/model_data/detectron_model.yaml’ saved [918/918]\n","\n","--2019-08-06 06:55:36--  https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15544345 (15M) [application/gzip]\n","Saving to: ‘/content/model_data/detectron_weights.tar.gz’\n","\n","/content/model_data 100%[===================>]  14.82M  6.77MB/s    in 2.2s    \n","\n","2019-08-06 06:55:39 (6.77 MB/s) - ‘/content/model_data/detectron_weights.tar.gz’ saved [15544345/15544345]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l4S01cUqE3WJ","colab_type":"text"},"source":["## Now, install some particular dependencies"]},{"cell_type":"code","metadata":{"id":"rQCyXjYyFQzp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":558},"outputId":"5fa85961-b5fd-49ce-bbc9-39cb45f48bde","executionInfo":{"status":"ok","timestamp":1565074559575,"user_tz":-180,"elapsed":148996,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["# Install dependencies\n","!pip install ninja yacs cython matplotlib demjson\n","!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (1.9.0.post1)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.6/dist-packages (0.1.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.13)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n","Requirement already satisfied: demjson in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs) (3.13)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n","Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-5ym5b9be\n","  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-5ym5b9be\n","Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (41.0.1)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.13)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.0.3)\n","Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.16.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.5.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=275140 sha256=ef61046bde6f27540eeaf352a6835fd5c216b881b1cf669fd14d89b572e7668c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gb20pxn_/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n","Successfully built pycocotools\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HNW1WWWyFkB5","colab_type":"text"},"source":["## Install fastText for installing Pythia"]},{"cell_type":"code","metadata":{"id":"ke7OQtzeFV7z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":319},"outputId":"43fd4fa3-bc10-487e-ad62-0d1e40ce60e3","executionInfo":{"status":"ok","timestamp":1565074607485,"user_tz":-180,"elapsed":196880,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["%cd /content/\n","%rm -rf fastText\n","!git clone https://github.com/facebookresearch/fastText.git fastText\n","%cd /content/fastText\n","!pip install -e ."],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'fastText'...\n","remote: Enumerating objects: 11, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 3309 (delta 0), reused 8 (delta 0), pack-reused 3298\u001b[K\n","Receiving objects: 100% (3309/3309), 7.92 MiB | 5.32 MiB/s, done.\n","Resolving deltas: 100% (2070/2070), done.\n","/content/fastText\n","Obtaining file:///content/fastText\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (2.3.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (41.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (1.16.4)\n","Installing collected packages: fasttext\n","  Found existing installation: fasttext 0.9.1\n","    Can't uninstall 'fasttext'. No files were found to uninstall.\n","  Running setup.py develop for fasttext\n","Successfully installed fasttext\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ivSdn9BFFpxp","colab_type":"text"},"source":["## Install Pythia now"]},{"cell_type":"code","metadata":{"id":"FOHchoDW7yqa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":655},"outputId":"b9626eb4-e144-4dda-f346-a62c5f5ac9e2","executionInfo":{"status":"ok","timestamp":1565074622094,"user_tz":-180,"elapsed":211480,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["%cd /content/\n","%rm -rf pythia\n","!git clone https://github.com/facebookresearch/pythia.git pythia\n","%cd /content/pythia\n","# Don't modify torch version\n","!sed -i '/torch/d' requirements.txt\n","!pip install -e .\n","import sys\n","sys.path.append('/content/pythia')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'pythia'...\n","remote: Enumerating objects: 103, done.\u001b[K\n","remote: Counting objects: 100% (103/103), done.\u001b[K\n","remote: Compressing objects: 100% (66/66), done.\u001b[K\n","remote: Total 3513 (delta 40), reused 71 (delta 35), pack-reused 3410\n","Receiving objects: 100% (3513/3513), 6.57 MiB | 4.53 MiB/s, done.\n","Resolving deltas: 100% (2270/2270), done.\n","/content/pythia\n","Obtaining file:///content/pythia\n","Requirement already satisfied: tensorboardX==1.2 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (1.2)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (1.16.4)\n","Requirement already satisfied: tqdm==4.19.9 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (4.19.9)\n","Requirement already satisfied: demjson>=2.2 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (2.2.4)\n","Requirement already satisfied: GitPython>=2.1 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (2.1.13)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (3.13)\n","Requirement already satisfied: pytest==3.3.2 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (3.3.2)\n","Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (2.21.0)\n","Requirement already satisfied: fastText in /content/fastText/python/fasttext_module (from pythia==0.3) (0.9.1)\n","Requirement already satisfied: nltk==3.4.1 in /usr/local/lib/python3.6/dist-packages (from pythia==0.3) (3.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->pythia==0.3) (1.12.0)\n","Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->pythia==0.3) (3.7.1)\n","Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.1->pythia==0.3) (2.0.5)\n","Requirement already satisfied: pluggy<0.7,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest==3.3.2->pythia==0.3) (0.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest==3.3.2->pythia==0.3) (41.0.1)\n","Requirement already satisfied: attrs>=17.2.0 in /usr/local/lib/python3.6/dist-packages (from pytest==3.3.2->pythia==0.3) (19.1.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==3.3.2->pythia==0.3) (1.8.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->pythia==0.3) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->pythia==0.3) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->pythia==0.3) (2019.6.16)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->pythia==0.3) (1.24.3)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fastText->pythia==0.3) (2.3.0)\n","Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=2.1->pythia==0.3) (2.0.5)\n","Installing collected packages: pythia\n","  Found existing installation: pythia 0.3\n","    Can't uninstall 'pythia'. No files were found to uninstall.\n","  Running setup.py develop for pythia\n","Successfully installed pythia\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B5o-zqvuFxeR","colab_type":"text"},"source":["## Install maskrcnn-benchmark now"]},{"cell_type":"code","metadata":{"id":"V1ROyH7yG11V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"4671bc88-9079-496b-c64e-6cbc62dcffc0","executionInfo":{"status":"ok","timestamp":1565074630029,"user_tz":-180,"elapsed":219405,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["# Install maskrcnn-benchmark to extract detectron features\n","%cd /content\n","!git clone https://gitlab.com/meetshah1995/vqa-maskrcnn-benchmark.git\n","%cd /content/vqa-maskrcnn-benchmark\n","# Compile custom layers and build mask-rcnn backbone\n","!python setup.py build\n","!python setup.py develop\n","sys.path.append('/content/vqa-maskrcnn-benchmark')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n","fatal: destination path 'vqa-maskrcnn-benchmark' already exists and is not an empty directory.\n","/content/vqa-maskrcnn-benchmark\n","running build\n","running build_py\n","running build_ext\n","running develop\n","running egg_info\n","writing maskrcnn_benchmark.egg-info/PKG-INFO\n","writing dependency_links to maskrcnn_benchmark.egg-info/dependency_links.txt\n","writing top-level names to maskrcnn_benchmark.egg-info/top_level.txt\n","writing manifest file 'maskrcnn_benchmark.egg-info/SOURCES.txt'\n","running build_ext\n","copying build/lib.linux-x86_64-3.6/maskrcnn_benchmark/_C.cpython-36m-x86_64-linux-gnu.so -> maskrcnn_benchmark\n","Creating /usr/local/lib/python3.6/dist-packages/maskrcnn-benchmark.egg-link (link to .)\n","maskrcnn-benchmark 0.1 is already the active version in easy-install.pth\n","\n","Installed /content/vqa-maskrcnn-benchmark\n","Processing dependencies for maskrcnn-benchmark==0.1\n","Finished processing dependencies for maskrcnn-benchmark==0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8yx6FeDEF2sw","colab_type":"text"},"source":["## Demo\n","\n","The class handles everything from feature extraction, token extraction and predicting the answer"]},{"cell_type":"code","metadata":{"id":"UCD0nso8YelA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"83516b4a-fc38-4218-ece8-f64fe4c70744","executionInfo":{"status":"ok","timestamp":1565074631546,"user_tz":-180,"elapsed":220910,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["%cd /content/\n","import yaml\n","import cv2\n","import torch\n","import requests\n","import numpy as np\n","import gc\n","import torch.nn.functional as F\n","import pandas as pd\n","\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from IPython.display import display, HTML, clear_output\n","from ipywidgets import widgets, Layout\n","from io import BytesIO\n","\n","\n","from maskrcnn_benchmark.config import cfg\n","from maskrcnn_benchmark.layers import nms\n","from maskrcnn_benchmark.modeling.detector import build_detection_model\n","from maskrcnn_benchmark.structures.image_list import to_image_list\n","from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n","\n","\n","from pythia.utils.configuration import ConfigNode\n","from pythia.tasks.processors import VocabProcessor, VQAAnswerProcessor\n","from pythia.models.pythia import Pythia\n","from pythia.common.registry import registry\n","from pythia.common.sample import Sample, SampleList\n","\n","\n","class PythiaDemo:\n","  TARGET_IMAGE_SIZE = [448, 448]\n","  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n","  CHANNEL_STD = [0.229, 0.224, 0.225]\n","  \n","  def __init__(self):\n","    self._init_processors()\n","    self.pythia_model = self._build_pythia_model()\n","    self.detection_model = self._build_detection_model()\n","    self.resnet_model = self._build_resnet_model()\n","    \n","  def _init_processors(self):\n","    with open(\"/content/model_data/pythia.yaml\") as f:\n","      config = yaml.load(f)\n","    \n","    config = ConfigNode(config)\n","    # Remove warning\n","    config.training_parameters.evalai_inference = True\n","    registry.register(\"config\", config)\n","    \n","    self.config = config\n","    \n","    vqa_config = config.task_attributes.vqa.dataset_attributes.vqa2\n","    text_processor_config = vqa_config.processors.text_processor\n","    answer_processor_config = vqa_config.processors.answer_processor\n","    \n","    text_processor_config.params.vocab.vocab_file = \"/content/model_data/vocabulary_100k.txt\"\n","    answer_processor_config.params.vocab_file = \"/content/model_data/answers_vqa.txt\"\n","    # Add preprocessor as that will needed when we are getting questions from user\n","    self.text_processor = VocabProcessor(text_processor_config.params)\n","    self.answer_processor = VQAAnswerProcessor(answer_processor_config.params)\n","\n","    registry.register(\"vqa2_text_processor\", self.text_processor)\n","    registry.register(\"vqa2_answer_processor\", self.answer_processor)\n","    registry.register(\"vqa2_num_final_outputs\", \n","                      self.answer_processor.get_vocab_size())\n","    \n","  def _build_pythia_model(self):\n","    state_dict = torch.load('/content/model_data/pythia.pth')\n","    model_config = self.config.model_attributes.pythia\n","    model_config.model_data_dir = \"/content/\"\n","    model = Pythia(model_config)\n","    model.build()\n","    model.init_losses_and_metrics()\n","    \n","    if list(state_dict.keys())[0].startswith('module') and \\\n","       not hasattr(model, 'module'):\n","      state_dict = self._multi_gpu_state_to_single(state_dict)\n","          \n","    model.load_state_dict(state_dict)\n","    model.to(\"cuda\")\n","    model.eval()\n","    \n","    return model\n","  \n","  def _build_resnet_model(self):\n","    self.data_transforms = transforms.Compose([\n","        transforms.Resize(self.TARGET_IMAGE_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n","    ])\n","    resnet152 = models.resnet152(pretrained=True)\n","    resnet152.eval()\n","    modules = list(resnet152.children())[:-2]\n","    self.resnet152_model = torch.nn.Sequential(*modules)\n","    self.resnet152_model.to(\"cuda\")\n","  \n","  def _multi_gpu_state_to_single(self, state_dict):\n","    new_sd = {}\n","    for k, v in state_dict.items():\n","        if not k.startswith('module.'):\n","            raise TypeError(\"Not a multiple GPU state of dict\")\n","        k1 = k[7:]\n","        new_sd[k1] = v\n","    return new_sd\n","  \n","  def predict(self, url, question):\n","    with torch.no_grad():\n","      detectron_features = self.get_detectron_features(url)\n","      resnet_features = self.get_resnet_features(url)\n","\n","      sample = Sample()\n","\n","      processed_text = self.text_processor({\"text\": question})\n","      sample.text = processed_text[\"text\"]\n","      sample.text_len = len(processed_text[\"tokens\"])\n","\n","      sample.image_feature_0 = detectron_features\n","      sample.image_info_0 = Sample({\n","          \"max_features\": torch.tensor(100, dtype=torch.long)\n","      })\n","\n","      sample.image_feature_1 = resnet_features\n","\n","      sample_list = SampleList([sample])\n","      sample_list = sample_list.to(\"cuda\")\n","\n","      scores = self.pythia_model(sample_list)[\"scores\"]\n","      scores = torch.nn.functional.softmax(scores, dim=1)\n","      actual, indices = scores.topk(5, dim=1)\n","\n","      top_indices = indices[0]\n","      top_scores = actual[0]\n","\n","      probs = []\n","      answers = []\n","\n","      for idx, score in enumerate(top_scores):\n","        probs.append(score.item())\n","        answers.append(\n","            self.answer_processor.idx2word(top_indices[idx].item())\n","        )\n","    \n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","    return probs, answers\n","    \n","  \n","  def _build_detection_model(self):\n","\n","      cfg.merge_from_file('/content/model_data/detectron_model.yaml')\n","      cfg.freeze()\n","\n","      model = build_detection_model(cfg)\n","      checkpoint = torch.load('/content/model_data/detectron_model.pth', \n","                              map_location=torch.device(\"cpu\"))\n","\n","      load_state_dict(model, checkpoint.pop(\"model\"))\n","\n","      model.to(\"cuda\")\n","      model.eval()\n","      return model\n","  \n","  def get_actual_image(self, image_path):\n","      if image_path.startswith('http'):\n","          path = requests.get(image_path, stream=True).raw\n","      else:\n","          path = image_path\n","      \n","      return path\n","\n","  def _image_transform(self, image_path):\n","      path = self.get_actual_image(image_path)\n","\n","      img = Image.open(path)\n","      im = np.array(img).astype(np.float32)\n","      im = im[:, :, ::-1]\n","      im -= np.array([102.9801, 115.9465, 122.7717])\n","      im_shape = im.shape\n","      im_size_min = np.min(im_shape[0:2])\n","      im_size_max = np.max(im_shape[0:2])\n","      im_scale = float(800) / float(im_size_min)\n","      # Prevent the biggest axis from being more than max_size\n","      if np.round(im_scale * im_size_max) > 1333:\n","           im_scale = float(1333) / float(im_size_max)\n","      im = cv2.resize(\n","           im,\n","           None,\n","           None,\n","           fx=im_scale,\n","           fy=im_scale,\n","           interpolation=cv2.INTER_LINEAR\n","       )\n","      img = torch.from_numpy(im).permute(2, 0, 1)\n","      return img, im_scale\n","\n","\n","  def _process_feature_extraction(self, output,\n","                                 im_scales,\n","                                 feat_name='fc6',\n","                                 conf_thresh=0.2):\n","      batch_size = len(output[0][\"proposals\"])\n","      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n","      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n","      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n","      feats = output[0][feat_name].split(n_boxes_per_image)\n","      cur_device = score_list[0].device\n","\n","      feat_list = []\n","\n","      for i in range(batch_size):\n","          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n","          scores = score_list[i]\n","\n","          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n","\n","          for cls_ind in range(1, scores.shape[1]):\n","              cls_scores = scores[:, cls_ind]\n","              keep = nms(dets, cls_scores, 0.5)\n","              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n","                                           cls_scores[keep],\n","                                           max_conf[keep])\n","\n","          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n","          feat_list.append(feats[i][keep_boxes])\n","      return feat_list\n","\n","  def masked_unk_softmax(self, x, dim, mask_idx):\n","      x1 = F.softmax(x, dim=dim)\n","      x1[:, mask_idx] = 0\n","      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n","      y = x1 / x1_sum\n","      return y\n","   \n","  def get_resnet_features(self, image_path):\n","      path = self.get_actual_image(image_path)\n","      img = Image.open(path).convert(\"RGB\")\n","      img_transform = self.data_transforms(img)\n","      \n","      if img_transform.shape[0] == 1:\n","        img_transform = img_transform.expand(3, -1, -1)\n","      img_transform = img_transform.unsqueeze(0).to(\"cuda\")\n","      \n","      features = self.resnet152_model(img_transform).permute(0, 2, 3, 1)\n","      features = features.view(196, 2048)\n","      return features\n","    \n","  def get_detectron_features(self, image_path):\n","      im, im_scale = self._image_transform(image_path)\n","      img_tensor, im_scales = [im], [im_scale]\n","      current_img_list = to_image_list(img_tensor, size_divisible=32)\n","      current_img_list = current_img_list.to('cuda')\n","      with torch.no_grad():\n","          output = self.detection_model(current_img_list)\n","      feat_list = self._process_feature_extraction(output, im_scales, \n","                                                  'fc6', 0.2)\n","      return feat_list[0]\n","    "],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t3IanIVPt91G","colab_type":"text"},"source":["### If the command below fails with 'CUDNN_EXECUTION_FAILED', try rerunning the cell"]},{"cell_type":"code","metadata":{"id":"IwF36OmQ72ir","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"9df299a7-ed28-46e3-dc46-170a507f5be2","executionInfo":{"status":"ok","timestamp":1565075037954,"user_tz":-180,"elapsed":627307,"user":{"displayName":"Ancient Colors","photoUrl":"","userId":"07486088927151244582"}}},"source":["demo = PythiaDemo()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/pythia/pythia/.vector_cache/glove.6B.zip: 862MB [04:57, 2.90MB/s]                           \n","100%|█████████▉| 399714/400000 [01:03<00:00, 6337.39it/s]Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n","\n","  0%|          | 0/241530880 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 4636672/241530880 [00:00<00:05, 46348420.66it/s]\u001b[A\n","  7%|▋         | 15958016/241530880 [00:00<00:04, 56326942.47it/s]\u001b[A\n"," 11%|█         | 26558464/241530880 [00:00<00:03, 65534125.24it/s]\u001b[A\n"," 15%|█▌        | 37167104/241530880 [00:00<00:02, 74021869.83it/s]\u001b[A\n"," 20%|█▉        | 48209920/241530880 [00:00<00:02, 82120238.72it/s]\u001b[A\n","Exception in thread Thread-5:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_monitor.py\", line 62, in run\n","    for instance in self.tqdm_cls._instances:\n","  File \"/usr/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n","    for itemref in self.data:\n","RuntimeError: Set changed size during iteration\n","\n","100%|██████████| 241530880/241530880 [00:02<00:00, 99575146.00it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"CinwDLv5GLJI","colab_type":"text"},"source":["## Use the text fields below to ask a question on an image\n","\n","Image URL can be any http/https URL. We show top 5 predictions from Pythia. Confidence shows how confident Pythia model was about a particular prediction."]},{"cell_type":"code","metadata":{"id":"_55MoLFlhL4Y","colab_type":"code","outputId":"53933181-6d72-4773-85d5-bd6c183e35ea","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1CXWuMEsNWl__SVtPU3x17vTsQFFlXjL2"}},"source":["def init_widgets(url, question):\n","  image_text = widgets.Text(\n","    description=\"Image URL\", layout=Layout(minwidth=\"70%\")\n","  )\n","  question_text = widgets.Text(\n","      description=\"Question\", layout=Layout(minwidth=\"70%\")\n","  )\n","\n","  image_text.value = url\n","  question_text.value = question\n","  submit_button = widgets.Button(description=\"Ask Pythia!\")\n","\n","  display(image_text)\n","  display(question_text)\n","  display(submit_button)\n","\n","  submit_button.on_click(lambda b: on_button_click(\n","      b, image_text, question_text\n","  ))\n","  \n","  return image_text, question_text\n","  \n","def on_button_click(b, image_text, question_text):\n","  clear_output()\n","  image_path = demo.get_actual_image(image_text.value)\n","  image = Image.open(image_path)\n","  \n","  scores, predictions = demo.predict(image_text.value, question_text.value)\n","  scores = [score * 100 for score in scores]\n","  df = pd.DataFrame({\n","      \"Prediction\": predictions,\n","      \"Confidence\": scores\n","  })\n","  \n","  init_widgets(image_text.value, question_text.value)\n","  display(image)\n","  \n","  display(HTML(df.to_html()))\n"," \n","\n","image_text, question_text = init_widgets(\n","    \"http://images.cocodataset.org/train2017/000000505539.jpg\", \n","    \"where is this place?\"\n",")\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}